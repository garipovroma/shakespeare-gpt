{"cells":[{"cell_type":"markdown","metadata":{"id":"349zD-ur4BGP"},"source":["### Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"gO-2l4xFEPxk"},"source":["### Read data"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:08:55.411812Z","iopub.status.busy":"2023-09-26T10:08:55.411297Z","iopub.status.idle":"2023-09-26T10:08:55.843683Z","shell.execute_reply":"2023-09-26T10:08:55.842629Z","shell.execute_reply.started":"2023-09-26T10:08:55.411777Z"},"id":"AsPJeJ0qAzPv","trusted":true},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:08:55.851486Z","iopub.status.busy":"2023-09-26T10:08:55.848942Z","iopub.status.idle":"2023-09-26T10:08:57.676365Z","shell.execute_reply":"2023-09-26T10:08:57.675250Z","shell.execute_reply.started":"2023-09-26T10:08:55.851451Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Shakespeare_data.csv  alllines.txt  william-shakespeare-black-silhouette.jpg\n"]}],"source":["!ls /kaggle/input/shakespeare-plays"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:08:57.678079Z","iopub.status.busy":"2023-09-26T10:08:57.677724Z","iopub.status.idle":"2023-09-26T10:08:58.129764Z","shell.execute_reply":"2023-09-26T10:08:58.128778Z","shell.execute_reply.started":"2023-09-26T10:08:57.678046Z"},"id":"eZ_8EoamAwU_","trusted":true},"outputs":[],"source":["df = pd.read_csv('/kaggle/input/shakespeare-plays/Shakespeare_data.csv')\n","df = df.dropna() # drop lines which doesn't correspond to player, lines like \"ACT I\"\n","df = df.reset_index()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-09-26T10:08:58.133390Z","iopub.status.busy":"2023-09-26T10:08:58.132668Z","iopub.status.idle":"2023-09-26T10:08:58.182779Z","shell.execute_reply":"2023-09-26T10:08:58.181693Z","shell.execute_reply.started":"2023-09-26T10:08:58.133354Z"},"id":"NI9-kHTqCa_V","outputId":"c114b5de-fd94-4af4-f8f8-e3a4ad7158a7","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["df.shape = (105152, 7)\n","train_df.shape = (89380, 7)\n","test_df.shape = (15772, 7)\n"]}],"source":["print(f'df.shape = {df.shape}')\n","\n","first_test_elem = df.shape[0] * 0.85\n","\n","train_df = df[df.index < first_test_elem]\n","test_df = df[df.index >= first_test_elem]\n","\n","print(f'train_df.shape = {train_df.shape}')\n","print(f'test_df.shape = {test_df.shape}')\n","\n","# simple idea just to consider data as a continius stream of player lines\n","\n","train_df = train_df[['PlayerLine']]\n","test_df = test_df[['PlayerLine']]\n","\n","def write_text(df, file_name):\n","    with open(file_name, 'w') as fout:\n","        fout.write(' '.join(df['PlayerLine'].tolist()))\n","\n","write_text(train_df, 'train.txt')\n","write_text(test_df, 'test.txt')"]},{"cell_type":"markdown","metadata":{"id":"dSMw5tkVETdO"},"source":["### Transformers baseline"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-09-26T10:08:58.184748Z","iopub.status.busy":"2023-09-26T10:08:58.184405Z","iopub.status.idle":"2023-09-26T10:09:12.931493Z","shell.execute_reply":"2023-09-26T10:09:12.930173Z","shell.execute_reply.started":"2023-09-26T10:08:58.184715Z"},"id":"RFedAmRlErke","outputId":"80d09d3f-c189-4d23-c2e4-e750dcbae392","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.33.0)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n","Requirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"]}],"source":["!pip install transformers[torch] accelerate"]},{"cell_type":"markdown","metadata":{"id":"b5xDlO6EKPnf"},"source":["Imports"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:12.935684Z","iopub.status.busy":"2023-09-26T10:09:12.935332Z","iopub.status.idle":"2023-09-26T10:09:28.243360Z","shell.execute_reply":"2023-09-26T10:09:28.242277Z","shell.execute_reply.started":"2023-09-26T10:09:12.935655Z"},"id":"pHBc0lC7ES04","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","import math"]},{"cell_type":"markdown","metadata":{"id":"gQyah5hcKRNA"},"source":["Load model and tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:28.251686Z","iopub.status.busy":"2023-09-26T10:09:28.248683Z","iopub.status.idle":"2023-09-26T10:09:34.851992Z","shell.execute_reply":"2023-09-26T10:09:34.850883Z","shell.execute_reply.started":"2023-09-26T10:09:28.251630Z"},"id":"WRmk2I9gFi9k","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eef947cb811b4e3684ee50adfc47328e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2dd5d8385b3b49dfa1fcb65c98c184a4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dce403fa051a4b688cdbd4f58d4e7b98","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e4d6e8bcfb424d1599ba833702773e7e","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fb76783e28ff46c9b911cfd75e19e0d4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load the pre-trained GPT-2 model and tokenizer\n","model_name = \"gpt2\"  # Choose an appropriate model size\n","# model_name = \"garipovroma/gpt_2_shakespeare_finetuned-1\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"KTVo0VzpKTdC"},"source":["Define train and val datasets"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:34.854620Z","iopub.status.busy":"2023-09-26T10:09:34.853880Z","iopub.status.idle":"2023-09-26T10:09:45.261429Z","shell.execute_reply":"2023-09-26T10:09:45.260412Z","shell.execute_reply.started":"2023-09-26T10:09:34.854585Z"},"id":"pNYpzQywFlY3","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]}],"source":["train_dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path='train.txt',  # Replace with the actual path\n","    block_size=400  # Adjust the block size as needed\n",")\n","\n","val_dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path='test.txt',  # Replace with the actual path\n","    block_size=400  # Adjust the block size as needed\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"jxvqa2RBKV1X"},"source":["Training args"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:45.265459Z","iopub.status.busy":"2023-09-26T10:09:45.264001Z","iopub.status.idle":"2023-09-26T10:09:45.343267Z","shell.execute_reply":"2023-09-26T10:09:45.342234Z","shell.execute_reply.started":"2023-09-26T10:09:45.265420Z"},"id":"7Fsx8Z1lFqm7","trusted":true},"outputs":[],"source":["# Fine-tuning arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./shakespeare_finetuned\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,  # Adjust the number of epochs as needed\n","    per_device_train_batch_size=8,  # Adjust batch size as needed\n","    per_device_eval_batch_size=8,\n","    evaluation_strategy=\"steps\",\n","    save_steps=50,\n","    eval_steps=10,\n","    logging_steps=10,  # Adjust logging frequency\n","    remove_unused_columns=False,  # Needed for custom metrics\n","#     save_total_limit=3,  # Limit the number of checkpoints saved\n",")\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:46.812306Z","iopub.status.busy":"2023-09-26T10:09:46.811906Z","iopub.status.idle":"2023-09-26T10:09:47.333163Z","shell.execute_reply":"2023-09-26T10:09:47.331873Z","shell.execute_reply.started":"2023-09-26T10:09:46.812275Z"},"id":"bPb4PnivF8IN","trusted":true},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","\n","    # Calculate perplexity\n","    loss = torch.nn.functional.cross_entropy(predictions.view(-1, predictions.size(-1)), labels.view(-1))\n","    perplexity = math.exp(loss.item())\n","\n","    # Convert predictions and labels to lists of strings\n","    predictions = [str(np.argmax(pred)) for pred in predictions]\n","    labels = [str(label.item()) for label in labels]\n","\n","    # Calculate BLEU score\n","    reference = [labels]  # Reference is a list of lists\n","    hypothesis = predictions\n","    bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=SmoothingFunction().method1)\n","\n","    return {\"perplexity\": perplexity, \"bleu\": bleu_score}"]},{"cell_type":"markdown","metadata":{"id":"0d3nFhgqKYFM"},"source":["Trainer"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:09:48.093878Z","iopub.status.busy":"2023-09-26T10:09:48.093162Z","iopub.status.idle":"2023-09-26T10:09:53.660224Z","shell.execute_reply":"2023-09-26T10:09:53.659218Z","shell.execute_reply.started":"2023-09-26T10:09:48.093842Z"},"id":"PJmSG-E9F4He","trusted":true},"outputs":[],"source":["# Create a Trainer for fine-tuning\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","#     compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{"id":"DyaP5RlpKZCl"},"source":["Run train loop"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":531},"execution":{"iopub.execute_input":"2023-09-26T10:09:53.663024Z","iopub.status.busy":"2023-09-26T10:09:53.662057Z","iopub.status.idle":"2023-09-26T10:27:38.158916Z","shell.execute_reply":"2023-09-26T10:27:38.157733Z","shell.execute_reply.started":"2023-09-26T10:09:53.662986Z"},"id":"R6U4UT_tIW_i","outputId":"d3469448-8b10-44aa-b731-8d10d1155e09","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.15.11 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.9"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230926_100958-wx576hgt</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/garipovroma/huggingface/runs/wx576hgt' target=\"_blank\">fresh-pine-11</a></strong> to <a href='https://wandb.ai/garipovroma/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/garipovroma/huggingface' target=\"_blank\">https://wandb.ai/garipovroma/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/garipovroma/huggingface/runs/wx576hgt' target=\"_blank\">https://wandb.ai/garipovroma/huggingface/runs/wx576hgt</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='426' max='426' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [426/426 16:58, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>4.665400</td>\n","      <td>4.479091</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>4.595800</td>\n","      <td>4.437127</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>4.479400</td>\n","      <td>4.402711</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>4.438700</td>\n","      <td>4.385350</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>4.455400</td>\n","      <td>4.367450</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>4.445100</td>\n","      <td>4.351386</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>4.406300</td>\n","      <td>4.338783</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>4.423300</td>\n","      <td>4.326476</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>4.384400</td>\n","      <td>4.321807</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>4.386100</td>\n","      <td>4.312300</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>4.355100</td>\n","      <td>4.302274</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>4.392500</td>\n","      <td>4.297254</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>4.341900</td>\n","      <td>4.293180</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>4.372200</td>\n","      <td>4.283644</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>4.287400</td>\n","      <td>4.280809</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>4.266800</td>\n","      <td>4.276367</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>4.283000</td>\n","      <td>4.274833</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>4.320000</td>\n","      <td>4.267859</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>4.241900</td>\n","      <td>4.267335</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>4.251400</td>\n","      <td>4.262408</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>4.277100</td>\n","      <td>4.258518</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>4.236600</td>\n","      <td>4.258040</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>4.289600</td>\n","      <td>4.253327</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>4.223100</td>\n","      <td>4.252250</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>4.237000</td>\n","      <td>4.248210</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>4.245300</td>\n","      <td>4.244993</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>4.229600</td>\n","      <td>4.245913</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>4.243400</td>\n","      <td>4.241535</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>4.250600</td>\n","      <td>4.239636</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>4.208100</td>\n","      <td>4.240572</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>4.197400</td>\n","      <td>4.238306</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>4.159800</td>\n","      <td>4.236850</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>4.208900</td>\n","      <td>4.235329</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>4.171700</td>\n","      <td>4.235316</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>4.190000</td>\n","      <td>4.233179</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>4.196900</td>\n","      <td>4.232438</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>4.180400</td>\n","      <td>4.232352</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>4.211900</td>\n","      <td>4.231936</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>4.187500</td>\n","      <td>4.230804</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>4.167900</td>\n","      <td>4.230398</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>4.212900</td>\n","      <td>4.230375</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>4.164500</td>\n","      <td>4.230065</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["# Fine-tune the model\n","trainer.train()\n","\n","# Save the model after training\n","model.save_pretrained(\"./shakespeare_finetuned\")\n","\n","# You can continue to generate text using the fine-tuned model as needed\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:02:33.588854Z","iopub.status.busy":"2023-09-26T10:02:33.588416Z","iopub.status.idle":"2023-09-26T10:02:47.739897Z","shell.execute_reply":"2023-09-26T10:02:47.738547Z","shell.execute_reply.started":"2023-09-26T10:02:33.588821Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2befcc6e6b5443de8f85b615f6308566","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:844: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/garipovroma/gpt_2_shakespeare_finetuned-2-400/commit/d562f8e87bb73613f5f81bd2f2471fd08bd59180', commit_message='Upload tokenizer', commit_description='', oid='d562f8e87bb73613f5f81bd2f2471fd08bd59180', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.push_to_hub(\"gpt_2_shakespeare_finetuned-2-400\", use_auth_token=\"\")\n","tokenizer.push_to_hub(\"gpt_2_shakespeare_finetuned-2-400\", use_auth_token=\"\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:02:47.743225Z","iopub.status.busy":"2023-09-26T10:02:47.742038Z","iopub.status.idle":"2023-09-26T10:02:47.750728Z","shell.execute_reply":"2023-09-26T10:02:47.749747Z","shell.execute_reply.started":"2023-09-26T10:02:47.743186Z"},"trusted":true},"outputs":[{"data":{"text/plain":["213"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["213"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-25T22:30:08.984025Z","iopub.status.idle":"2023-09-25T22:30:08.985043Z","shell.execute_reply":"2023-09-25T22:30:08.984740Z","shell.execute_reply.started":"2023-09-25T22:30:08.984710Z"},"trusted":true},"outputs":[],"source":["# !zip shakespeare_finetuned_400_0.zip -r shakespeare_finetuned"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:41:29.877824Z","iopub.status.busy":"2023-09-26T10:41:29.877414Z","iopub.status.idle":"2023-09-26T10:41:31.035147Z","shell.execute_reply":"2023-09-26T10:41:31.033903Z","shell.execute_reply.started":"2023-09-26T10:41:29.877793Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint-100\tcheckpoint-250\tcheckpoint-400\tgeneration_config.json\n","checkpoint-150\tcheckpoint-300\tcheckpoint-50\tpytorch_model.bin\n","checkpoint-200\tcheckpoint-350\tconfig.json\truns\n"]}],"source":["!cd shakespeare_finetuned && ls"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:41:50.890041Z","iopub.status.busy":"2023-09-26T10:41:50.889659Z","iopub.status.idle":"2023-09-26T10:50:31.142673Z","shell.execute_reply":"2023-09-26T10:50:31.141583Z","shell.execute_reply.started":"2023-09-26T10:41:50.889990Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: shakespeare_finetuned/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-250/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-250/scheduler.pt (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-250/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-250/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-250/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-250/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-250/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-250/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-250/trainer_state.json (deflated 82%)\n","  adding: shakespeare_finetuned/checkpoint-300/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-300/scheduler.pt (deflated 48%)\n","  adding: shakespeare_finetuned/checkpoint-300/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-300/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-300/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-300/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-300/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-300/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-300/trainer_state.json (deflated 83%)\n","  adding: shakespeare_finetuned/checkpoint-50/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-50/scheduler.pt (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-50/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-50/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-50/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-50/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-50/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-50/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-50/trainer_state.json (deflated 73%)\n","  adding: shakespeare_finetuned/runs/ (stored 0%)\n","  adding: shakespeare_finetuned/runs/Sep26_10-09-45_fb7b3f44fe62/ (stored 0%)\n","  adding: shakespeare_finetuned/runs/Sep26_10-09-45_fb7b3f44fe62/events.out.tfevents.1695722993.fb7b3f44fe62.28.0 (deflated 67%)\n","  adding: shakespeare_finetuned/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-100/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-100/scheduler.pt (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-100/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-100/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-100/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-100/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-100/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-100/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-100/trainer_state.json (deflated 78%)\n","  adding: shakespeare_finetuned/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-350/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-350/scheduler.pt (deflated 48%)\n","  adding: shakespeare_finetuned/checkpoint-350/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-350/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-350/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-350/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-350/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-350/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-350/trainer_state.json (deflated 83%)\n","  adding: shakespeare_finetuned/checkpoint-150/ (stored 0%)\n","  adding: shakespeare_finetuned/checkpoint-150/scheduler.pt (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-150/training_args.bin (deflated 49%)\n","  adding: shakespeare_finetuned/checkpoint-150/optimizer.pt (deflated 8%)\n","  adding: shakespeare_finetuned/checkpoint-150/generation_config.json (deflated 24%)\n","  adding: shakespeare_finetuned/checkpoint-150/rng_state.pth (deflated 28%)\n","  adding: shakespeare_finetuned/checkpoint-150/config.json (deflated 51%)\n","  adding: shakespeare_finetuned/checkpoint-150/pytorch_model.bin (deflated 7%)\n","  adding: shakespeare_finetuned/checkpoint-150/trainer_state.json (deflated 80%)\n","  adding: shakespeare_finetuned/pytorch_model.bin\n","zip I/O error: No space left on device\n","zip error: Output file write failure (write error on zip file)\n"]}],"source":["!zip shakespeare_finetuned.zip -r shakespeare_finetuned/"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:50:31.145375Z","iopub.status.busy":"2023-09-26T10:50:31.145039Z","iopub.status.idle":"2023-09-26T10:50:32.219524Z","shell.execute_reply":"2023-09-26T10:50:32.218418Z","shell.execute_reply.started":"2023-09-26T10:50:31.145347Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["total 7.0M\n","-rw-r--r--  1 root root 447K Sep 26 10:09 cached_lm_GPT2Tokenizer_400_test.txt\n","-rw-r--r--  1 root root    0 Sep 26 10:09 cached_lm_GPT2Tokenizer_400_test.txt.lock\n","-rw-r--r--  1 root root 2.5M Sep 26 10:09 cached_lm_GPT2Tokenizer_400_train.txt\n","-rw-r--r--  1 root root    0 Sep 26 10:09 cached_lm_GPT2Tokenizer_400_train.txt.lock\n","drwxr-xr-x 11 root root 4.0K Sep 26 10:27 shakespeare_finetuned\n","-rw-r--r--  1 root root 615K Sep 26 10:08 test.txt\n","-rw-r--r--  1 root root 3.5M Sep 26 10:08 train.txt\n","drwxr-xr-x  3 root root 4.0K Sep 26 10:09 wandb\n"]},{"name":"stderr","output_type":"stream","text":["Thread WriterThread:\n","Traceback (most recent call last):\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 49, in run\n","    self._run()\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 100, in _run\n","    self._process(record)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 380, in _process\n","    self._wm.write(record)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 154, in write\n","    write_handler(record)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 135, in _write\n","    self._write_record(record)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/writer.py\", line 109, in _write_record\n","    ret = self._ds.write(record)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 289, in write\n","    ret = self._write_data(s)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 245, in _write_data\n","    self._write_record(s)\n","  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/internal/datastore.py\", line 224, in _write_record\n","    self._fp.write(s)\n","OSError: [Errno 28] No space left on device\n","wandb: ERROR Internal wandb error: file data was not synced\n"]}],"source":["!ls -lh"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-25T22:30:08.988736Z","iopub.status.idle":"2023-09-25T22:30:08.989661Z","shell.execute_reply":"2023-09-25T22:30:08.989398Z","shell.execute_reply.started":"2023-09-25T22:30:08.989373Z"},"trusted":true},"outputs":[],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-25T22:30:08.991110Z","iopub.status.idle":"2023-09-25T22:30:08.992023Z","shell.execute_reply":"2023-09-25T22:30:08.991745Z","shell.execute_reply.started":"2023-09-25T22:30:08.991718Z"},"trusted":true},"outputs":[],"source":["# tokenizer.push_to_hub(\"gpt_2_shakespeare_finetuned\", use_auth_token=\"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\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-09-25T22:30:08.994145Z","iopub.status.idle":"2023-09-25T22:30:08.995002Z","shell.execute_reply":"2023-09-25T22:30:08.994729Z","shell.execute_reply.started":"2023-09-25T22:30:08.994703Z"},"trusted":true},"outputs":[],"source":["# train_df"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:07:05.008586Z","iopub.status.busy":"2023-09-26T10:07:05.008172Z","iopub.status.idle":"2023-09-26T10:07:09.362681Z","shell.execute_reply":"2023-09-26T10:07:09.361538Z","shell.execute_reply.started":"2023-09-26T10:07:05.008556Z"},"trusted":true},"outputs":[],"source":["model = GPT2LMHeadModel.from_pretrained(\"shakespeare_finetuned/checkpoint-400/\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:06:14.606972Z","iopub.status.busy":"2023-09-26T10:06:14.606564Z","iopub.status.idle":"2023-09-26T10:06:15.733208Z","shell.execute_reply":"2023-09-26T10:06:15.731226Z","shell.execute_reply.started":"2023-09-26T10:06:14.606937Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[".\t     generation_config.json  rng_state.pth\t training_args.bin\n","..\t     optimizer.pt\t     scheduler.pt\n","config.json  pytorch_model.bin\t     trainer_state.json\n"]}],"source":["!ls -a shakespeare_finetuned/checkpoint-400"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T10:07:11.243947Z","iopub.status.busy":"2023-09-26T10:07:11.243369Z","iopub.status.idle":"2023-09-26T10:07:16.822614Z","shell.execute_reply":"2023-09-26T10:07:16.821547Z","shell.execute_reply.started":"2023-09-26T10:07:11.243907Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/plain":["[{'generated_text': \"To be or not to be, no true soldier? So, if she be not worthy of the soldier's command, yet be worthy of him: here is another letter. Ay, yet I had rather than a little. Ay, yet I had rather than a dozen, than a soldier, yet a slave, yet a fool, yet a slave, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet a fool, yet\"},\n"," {'generated_text': \"To be or not to be? What means't you here for? What do you say of a man that will do her harm? Well I told her, I have to look after that you do. My lord, I am your father, good sir. Nay, I'll give you the truth of this matter, my father is your husband and I shall take no interest in her if he will not. What if he is the heir? Nay, we'll take no interest in him: he must lose the king with his wife and he shall be heir of that king. So do'ster\"},\n"," {'generated_text': \"To be or not to be: this cannot be. I have come so far, Lord Talbot, that I can do good by doing my part. Do not fall mad, Sir John, but say, Thou have not thought of the cause of this. Sir John, go you to my uncle's bed. Ay, sir: he will let me tell you. He will take care of me too, Ay, sir. I do love him. I do love his majesty. I do love the lords and gentlemen of my honour. You should kiss all the ladies that I have loved! [\"},\n"," {'generated_text': 'To be or not to be this. How now? Who says? What! What?--what say you, sirrah? It is a very great one indeed, sirrah. There is a woman in general. What did the prince call her? She is my servant, I know you, well known here. I will see some of your letters, sirrah. You speak well of her, too, sirrah, for so are I. Do you remember your king? Nay, sirrah, but when I was young. I came thence to France, I hope, as far as'},\n"," {'generated_text': \"To be or not to be: then go, as I shall and will, To the sea without any man's sword, No less an officer than his own life To protect us Than the Lord Protector himself shall, for our life, make. How comes this of thee? You, like a good king, Do warrant my lordship not: That the people of your dominions Which have sworn to enforce this war should not, Should by reason of your authority refuse this royal office, And in what way that he now usurpeth, Shall stand without him and stand without her? If, on\"}]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# Move the model to CUDA (GPU) if available\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","from transformers import pipeline, set_seed\n","generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n","set_seed(42)\n","generator(\"To be or not to be\", max_length=120, num_return_sequences=5)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T09:21:05.566123Z","iopub.status.busy":"2023-09-26T09:21:05.565745Z","iopub.status.idle":"2023-09-26T09:21:07.514176Z","shell.execute_reply":"2023-09-26T09:21:07.513194Z","shell.execute_reply.started":"2023-09-26T09:21:05.566093Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"data":{"text/plain":["[{'generated_text': \"The weather today is fine! A good deal of this weather: some of it is that dares not touch the soil, and so no doubt comes from that the dusky sun bears a little light: It bears so little, indeed, in a pinch as is that much, being most heavy on one foot. When this news comes, that the world was no longer the world, let me tell my noble brother that this morning is well. Now do you know, my lord? I, my lady, here confess'd that the world had no soul to endure so much as this, so my\"},\n"," {'generated_text': 'The weather today is fine! If we hold that peace, it is so, I am sure, we will make for York, and henceforth we shall be glad. But what say your peers! Come, your lordship, my lords, there is the clerk, who hath gone with us to the court: but, my lord, my noble lord, he hath not gone. Let it be known to you, gentleman, that, in this state of peril, I think the better way may be known. And to his good master, your noble lord, I fear you well: I am'},\n"," {'generated_text': 'The weather today is fine! Here is a woman to go there with you, And I, at my word, would have to make the worst. Why, a man. Well, then, my dear, I pray thee, leave it to be this. Farewell, my lord: I am done, if need be, When thou vouchsafe me the time. Where is your servant? Come, sir. I will send your messenger back With news in hand. To be merry, and do thee honour well. You, sir, and I, the first morning, were all in a'},\n"," {'generated_text': \"The weather today is fine! My sister Cassius, you shall see how she sleeps. To be my queen, thou art mistress of your estate and mistress of your estate. [Aside]  We are bound, though we go with our countrymen, To keep our laws as we are to be us: our subjects should live by our will, So much as we hold with our means! My noble duke, I love a gentleman so highly I would have him. I will I would have had it so, and, if I may think it worth that, The duke's daughter,\"},\n"," {'generated_text': \"The weather today is fine! I'll go to the morrow, and the moon will shine, Will this moon glow? Or may I go, and see the moon? Come, my lord, what say you? Why tell us, my lord, What say you? and do with haste. Come, my lord, we'll come: This is a man, and we are men. I can swear he will not say'so,' but 'he' must be true. Say for the woman. He hath some wit, and it may be she hath no wit: So shall it suffice\"}]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["generator(\"The weather today is fine!\", max_length=120, num_return_sequences=5)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-09-26T09:23:03.756582Z","iopub.status.busy":"2023-09-26T09:23:03.756159Z","iopub.status.idle":"2023-09-26T09:23:03.766568Z","shell.execute_reply":"2023-09-26T09:23:03.765262Z","shell.execute_reply.started":"2023-09-26T09:23:03.756553Z"},"trusted":true},"outputs":[{"data":{"text/plain":["485"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["len(\"The weather today is fine! A good deal of this weather: some of it is that dares not touch the soil, and so no doubt comes from that the dusky sun bears a little light: It bears so little, indeed, in a pinch as is that much, being most heavy on one foot. When this news comes, that the world was no longer the world, let me tell my noble brother that this morning is well. Now do you know, my lord? I, my lady, here confess'd that the world had no soul to endure so much as this, so my\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["349zD-ur4BGP"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
